{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"senin token\"  # Hugging Face üzerinden ince ayar için kullanacağımız modeli indirmek için Hugging Face tokenı gerekmektedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneyi içe aktarma\n",
    "from unsloth import FastLanguageModel  # Unsloth kütüphanesinden hızlı model yükleme sınıfını içe aktarır, büyük dil modellerini optimize eder\n",
    "\n",
    "# PyTorch kütüphanesini içe aktarma\n",
    "import torch                           # PyTorch'u tensör işlemleri ve model hesaplamaları için içe aktarır\n",
    "\n",
    "# Maksimum dizi uzunluğu ayarı\n",
    "max_seq_length = 2048                  # Modelin bir seferde işleyebileceği maksimum token sayısını 2048 olarak tanımlar, RoPE Scaling ile otomatik ölçeklenir\n",
    "\n",
    "# Veri tipi ayarı\n",
    "#dtype = None                           # Veri tipini otomatik algılamaya bırakır; Tesla T4/V100 için Float16, Ampere+ için Bfloat16 seçilir\n",
    "dtype = torch.bfloat16 #A5000 ekran kartı için bunu seçtik.\n",
    "\n",
    "# 4-bit kuantizasyon ayarı\n",
    "load_in_4bit = True                    # Modeli 4-bit kuantizasyon ile yükler, bu bellek kullanımını azaltır ve performansı korur (False yapılırsa tam hassasiyet kullanılır)\n",
    "\n",
    "# 4-bit önceden kuantize edilmiş modellerin listesi\n",
    "fourbit_models = [                     # Unsloth’un desteklediği, 4-bit kuantize edilmiş modellerin listesi; hızlı indirme ve bellek tasarrufu sağlar\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 8B modeli, 15 trilyon token ile eğitilmiş, 2 kat hızlı\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",  # Llama-3.1 8B’nin talimatlara özel sürümü\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",     # Llama-3.1 70B modeli\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # Llama-3.1 405B modeli, 4-bit olarak yükleniyor\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # Mistral Nemo temel modeli, 12B, 2 kat hızlı\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",  # Mistral Nemo’nun talimat sürümü\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral 7B v0.3, 2 kat hızlı\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",   # Mistral 7B v0.3 talimat sürümü\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 mini, talimatlara özel, 2 kat hızlı\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",        # Phi-3 orta boy, 4k token kapasiteli\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",             # Gemma 2 9B modeli\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2 27B modeli, 2 kat hızlı\n",
    "]  # Daha fazla model için: https://huggingface.co/unsloth\n",
    "model_path = \"deepseek-14B indirdiğimiz model yolu\"\n",
    "\n",
    "# Modeli ve tokenizer'ı önceden eğitilmiş olarak yükleme\n",
    "model, tokenizer = FastLanguageModel.from_pretrained( \n",
    "   # force_download=True,\n",
    "      # Modeli ve tokenizer'ı yükler, Unsloth optimizasyonlarıyla\n",
    "    model_name=model_path,   # Yüklenecek modelin adı: LLaMA-3.1 8B (8 milyar parametre)\n",
    "    max_seq_length=max_seq_length,            # Maksimum dizi uzunluğunu 2048 olarak ayarlar\n",
    "    dtype=dtype,                              # Veri tipini otomatik algılamaya bırakır (Tesla T4 için Float16 olur)\n",
    "    load_in_4bit=load_in_4bit,                # 4-bit kuantizasyonu etkinleştirir, belleği optimize eder\n",
    "    #token = \"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\",                       # Eğer kısıtlı erişimli bir model kullanıyorsanız Hugging Face token’ı eklenir (burada pasif)\n",
    ")  # Model ve tokenizer nesnelerini döndürür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR =  \"kayıt edilecek yer\" #Model verilerini kayıt edeceğiniz klasörü seçin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli LoRA ile yapılandırma\n",
    "model = FastLanguageModel.get_peft_model(  # Modeli LoRA ile optimize edilmiş bir PEFT (Parameter-Efficient Fine-Tuning) modeline dönüştürür\n",
    "    model,                                 # LoRA'nın uygulanacağı ana model (önceki adımda yüklenen model)\n",
    "    r=32,                                  # LoRA'nın rank değeri: 16’dan 32’ye artırıldı (RTX A5000’in 24 GB VRAM’i daha yüksek rank için yeterli, daha iyi performans için) eskisi ise 16\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # LoRA'nın uygulanacağı Transformer katmanları (dikkat mekanizması ve feed-forward katmanları, değişmedi)\n",
    "    lora_alpha=32,                         # LoRA'nın ölçeklendirme faktörü: 16’dan 32’ye artırıldı (r ile uyumlu, daha güçlü öğrenme için) eskisi ise 16\n",
    "    lora_dropout=0.05,                     # LoRA için dropout oranı: 0’dan 0.05’e artırıldı (büyük veri setinde overfitting riskine karşı) eskisi ise 0\n",
    "    bias=\"none\",                           # Bias ayarı: \"none\" (değişmedi, optimize edilmiş bir seçenek)\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Gradient checkpointing ayarı: \"unsloth\" (değişmedi, VRAM’i %30 azaltır, 2 kat büyük batch size sağlar)\n",
    "    random_state=3407,                     # Rastgele durum tohumu: 3407 (değişmedi, tekrarlanabilirlik için sabit)\n",
    "    use_rslora=True,                       # Rank Stabilized LoRA (RSLoRA) kullanımı: False’tan True’ya değiştirildi (stabiliteyi artırır, büyük veri setlerinde faydalı) eskisi ise False\n",
    "    loftq_config=None,                     # LoftQ (Low-Rank Factorized Quantization) yapılandırması: None (değişmedi, ihtiyaç yoksa ek optimizasyon gerekmez)\n",
    ")                                          # LoRA ile yapılandırılmış yeni modeli döndürür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca formatı için prompt şablonu tanımlama\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"  # Üç bölümden oluşan bir şablon: talimat (instruction), giriş (input) ve cevap (response) yer tutucuları içerir\n",
    "\n",
    "# EOS_TOKEN tanımlama\n",
    "EOS_TOKEN = tokenizer.eos_token  # Tokenizer'dan EOS (End of Sequence) token'ını alır, metnin sonunu işaretler\n",
    "\n",
    "# Veri setini biçimlendirme fonksiyonu\n",
    "def formatting_prompts_func(examples):  # Veri setindeki örnekleri Alpaca formatına dönüştürmek için bir fonksiyon\n",
    "    instructions = examples[\"instruction\"]  # Veri setindeki \"instruction\" sütununu alır (talimatlar)\n",
    "    inputs       = examples[\"input\"]        # Veri setindeki \"input\" sütununu alır (girişler)\n",
    "    outputs      = examples[\"output\"]       # Veri setindeki \"output\" sütununu alır (cevaplar)\n",
    "    texts = []                              # Biçimlendirilmiş metinleri saklamak için boş bir liste oluşturur\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):  # Her bir talimat, giriş ve cevap üçlüsünü eşleştirir\n",
    "        # Alpaca şablonunu doldurur ve EOS_TOKEN ekler, yoksa üretim sonsuza dek devam eder!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN  # Şablonu doldurur ve metnin sonuna EOS token’ı ekler\n",
    "        texts.append(text)                  # Biçimlendirilmiş metni listeye ekler\n",
    "    return { \"text\" : texts, }              # \"text\" anahtarıyla biçimlendirilmiş metinleri bir sözlük olarak döndürür\n",
    "pass                                       # Boş bir \"pass\" ifadesi (gereksiz, fonksiyon zaten tamamlandı)\n",
    "\n",
    "# Veri setini yükleme ve işleme\n",
    "from datasets import load_dataset     \n",
    "from datasets import load_from_disk\n",
    "  # Hugging Face’in datasets kütüphanesini içe aktarır\n",
    "dataset = load_dataset(\"json\", data_files=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\alpaca_format.json\")[\"train\"]  # JSON dosyasından veri setini yükler ve \"train\" bölümünü alır\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True,)  # Veri setine biçimlendirme fonksiyonunu toplu (batched) şekilde uygular\n",
    "\n",
    "#tokenized_dataset = load_from_disk(f\"{CHECKPOINT_DIR}/tokenized_dataset\")\n",
    "\n",
    "# Eğitim için dataset'i ayarla\n",
    "#dataset = tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def add_attention_mask(example):\n",
    "    example[\"attention_mask\"] = [1] * len(example[\"input_ids\"])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_attention_mask, num_proc=4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneleri içe aktarma\n",
    "from trl import SFTTrainer              # TRL kütüphanesinden SFTTrainer’ı içe aktarır, denetimli ince ayar için kullanılır\n",
    "from transformers import TrainingArguments  # Hugging Face Transformers’tan eğitim argümanlarını içe aktarır\n",
    "from unsloth import is_bfloat16_supported   # Unsloth’tan bfloat16 desteğini kontrol eden bir fonksiyon alır\n",
    "\n",
    "# Eğitim sürecini başlatmak için trainer nesnesi oluşturma\n",
    "trainer = SFTTrainer(                   # Modeli eğitmek için SFTTrainer nesnesi oluşturur\n",
    "    model=model,                        # Eğitilecek model (önceki adımlarda LoRA ile yapılandırılmış)\n",
    "    tokenizer=tokenizer,                # Modelin tokenizer’ı (metni token’lara çevirir)\n",
    "    train_dataset=dataset,              # Eğitim için kullanılacak veri seti (Alpaca formatında işlenmiş)\n",
    "    dataset_text_field=\"text\",          # Veri setindeki hangi sütunun kullanılacağı: \"text\" (biçimlendirilmiş prompt’lar)\n",
    "    max_seq_length=2048,                # Maksimum dizi uzunluğu (önceki adımda 2048 olarak tanımlı, uzun bağlamlar için yeterli)\n",
    "    dataset_num_proc=4,                 # Veri işleme için kullanılacak işlemci sayısı: 4 (RTX A5000’in gücüyle paralel işleme artırıldı, 5 milyon satır için daha hızlı)\n",
    "    packing=True,                       # Veri paketlemeyi etkinleştirir; kısa diziler için 5x hız artışı sağlar, büyük veri setinde faydalı\n",
    "    args=TrainingArguments(             # Eğitim argümanlarını tanımlar\n",
    "        per_device_train_batch_size=8,  # Her cihaz (GPU) için eğitim batch boyutu: 8 (24 GB VRAM ile mümkün, performans artırıldı)\n",
    "        gradient_accumulation_steps=4,  # Gradyan biriktirme adımları: 4 (efektif batch size’ı 8*4=32 yapar, bellek yönetimi için)\n",
    "        warmup_steps=100,               # Öğrenme oranı ısınma adımları: 100 (büyük veri seti için daha yavaş ve stabil ısınma, 5’ten artırıldı)\n",
    "        num_train_epochs=2,           # Tam bir epoch için eğitim (yorum satırı, şu anda kullanılmıyor, veri seti büyük olduğu için adım bazlı tercih ediliyor)\n",
    "        max_steps=1500,                 # Toplam eğitim adımı sayısı: 1000 (60’tan artırıldı, 5 milyon satır için yeterli öğrenme sağlamak için)\n",
    "        learning_rate=2e-4,             # Öğrenme oranı: 0.0002 (modelin ne kadar hızlı öğreneceğini belirler, büyük veri setinde uygun)\n",
    "        fp16=not is_bfloat16_supported(),  # Float16 kullanımı: bfloat16 desteklenmiyorsa True (RTX A5000’de bfloat16 destekleniyor)\n",
    "        bf16=is_bfloat16_supported(),   # Bfloat16 kullanımı: destekleniyorsa True (Ampere mimarisi için RTX A5000’de etkin, daha verimli)\n",
    "        logging_steps=10,               # Log kayıt sıklığı: her 10 adımda bir (1’den artırıldı, büyük veri setinde log sıklığını azaltır)\n",
    "        optim=\"adamw_8bit\",             # Optimize edici: 8-bit AdamW (bellek verimli bir versiyon, değişmedi)\n",
    "        weight_decay=0.01,              # Ağırlık çürümesi: 0.01 (overfitting’i önlemek için regularization, değişmedi)\n",
    "        lr_scheduler_type=\"linear\",     # Öğrenme oranı zamanlayıcısı: doğrusal (lineer bir şekilde azalır, değişmedi)\n",
    "        seed=3407,                      # Rastgele tohum: 3407 (tekrarlanabilirlik için, değişmedi)\n",
    "        output_dir=CHECKPOINT_DIR,      # Çıktıların kaydedileceği dizin: \"CHECKPOINT_DIR\" (değişmedi)\n",
    "        report_to=\"none\",               # Eğitim raporlama: \"none\" (WandB gibi araçlar kullanılmayacak, değişmedi)\n",
    "    ),                                  # Eğitim argümanlarını tamamlar\n",
    ")                                       # Trainer nesnesini oluşturur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token’lanmış veri setini kaydet\n",
    "trainer.train_dataset.save_to_disk(f\"{CHECKPOINT_DIR}/tokenized_dataset2_atten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# 9. Eğitimi Başlat\n",
    "# ----------------------------------------------\n",
    "try:                                    # Hata yakalama bloğu başlatır, checkpoint ile devam etmeyi dener\n",
    "    print(\"Checkpoint aranıyor...\")     # Kullanıcıya checkpoint’in arandığını bildirir\n",
    "    trainer.train(resume_from_checkpoint=True)  # Eğitime son checkpoint’ten devam etmeyi dener (önceki bir eğitim varsa)\n",
    "except Exception as e:                  # Eğer bir hata oluşursa (örneğin, checkpoint bulunamazsa) bu blok çalışır\n",
    "    print(f\"Hata: {str(e)[:200]}...\\nYeni eğitim başlatılıyor...\")  # Hatanın ilk 200 karakterini yazdırır ve yeni eğitime geçeceğini bildirir\n",
    "    trainer.train()                     # Sıfırdan yeni bir eğitim başlatır (checkpoint olmadan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model eğitimini durdurmak gerekiyordu çünkü 26 saat istedi.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 step için 26 saat verdi ve 137,625,600 parametre eğitilecek. Bunu hafta sonu 2000 step için 52 saat+3 saat tokenizer olarak yaptığımızda 274 milyon parametre eğitilecek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"model yolu\",  # Path to the base model\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca formatı için prompt şablonu tanımlama\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"  # Üç bölümden oluşan bir şablon: talimat (instruction), giriş (input) ve cevap (response) yer tutucuları içerir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Fenerbahçe'nin son maçlarının sonuçlarını listele ve her maç için kısa bir açıklama yap.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Fenerbahçe'nin son maçları şunlardır:\n",
      "\n",
      " 19 Eylül 2014 - İstanbul - Fenerbahçe - Galatasaray - 3-1 - Süper Kupa\n",
      " 19 Eylül 2014 - İstanbul - Fenerbahçe - Adana Demirspor - 2-1 - Süper Lig\n",
      " 27 Eylül 2014 - İstanbul - Fenerbahçe - Gaziantepspor - 2-0 - Süper Lig\n",
      " 11 Ekim 2014 - İstanbul - Fenerbahçe - Trabzonspor - 1-0 - Süper Lig\n",
      " 15 Ekim 2014 - İstanbul - Fenerbahçe - Adana Demirspor - 3-1 - Süper Lig\n",
      " 19 Ekim 2014 - İstanbul - Fenerbahçe - Trabzonspor - 3-0 - Süper Lig\n",
      " 26 Ekim 2014 - İstanbul - Fenerbahçe - Sivasspor - 1-0 - Süper Lig\n",
      " 2 Kasım 2014 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 10 Kasım 2014 - İstanbul - Fenerbahçe - Galatasaray - 2-1 - Süper Lig\n",
      " 23 Kasım 2014 - İstanbul - Fenerbahçe - Trabzonspor - 3-1 - Süper Lig\n",
      " 26 Kasım 2014 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 8 Aralık 2014 - İstanbul - Fenerbahçe - Adana Demirspor - 2-0 - Süper Lig\n",
      " 13 Aralık 2014 - İstanbul - Fenerbahçe - Trabzonspor - 2-0 - Süper Lig\n",
      " 20 Aralık 2014 - İstanbul - Fenerbahçe - Sivasspor - 2-1 - Süper Lig\n",
      " 27 Aralık 2014 - İstanbul - Fenerbahçe - Kasımpaşa - 1-0 - Süper Lig\n",
      " 1 Ocak 2015 - İstanbul - Fenerbahçe - Trabzonspor - 1-1 - Süper Lig\n",
      " 4 Ocak 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 10 Ocak 2015 - İstanbul - Fenerbahçe - Trabzonspor - 2-0 - Süper Lig\n",
      " 25 Ocak 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 28 Ocak 2015 - İstanbul - Fenerbahçe - Sivasspor - 2-0 - Süper Lig\n",
      " 11 Şubat 2015 - İstanbul - Fenerbahçe - Trabzonspor - 3-1 - Süper Lig\n",
      " 14 Şubat 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 3-0 - Süper Lig\n",
      " 21 Şubat 2015 - İstanbul - Fenerbahçe - Trabzonspor - 2-0 - Süper Lig\n",
      " 28 Şubat 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 12 Mart 2015 - İstanbul - Fenerbahçe - Sivasspor - 1-1 - Süper Lig\n",
      " 16 Mart 2015 - İstanbul - Fenerbahçe - Trabzonspor - 1-1 - Süper Lig\n",
      " 22 Mart 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 26 Mart 2015 - İstanbul - Fenerbahçe - Trabzonspor - 3-1 - Süper Lig\n",
      " 29 Mart 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 3-0 - Süper Lig\n",
      " 12 Nisan 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 1-0 - Süper Lig\n",
      " 15 Nisan 2015 - İstanbul - Fenerbahçe - Trabzonspor - 3-2 - Süper Lig\n",
      " 22 Nisan 2015 - İstanbul - F\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "        alpaca_prompt.format(           # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"Fenerbahçe'nin son maçlarının sonuçlarını listele ve her maç için kısa bir açıklama yap.\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "            \"\",                         # Giriş: Boş bırakıldı, model kendi bilgisine dayanacak\n",
    "            \"\",                         # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=1024,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Çıktıyı decode etme ve yazdırma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Üretilen token’ları insan tarafından okunabilir metne çevirir, özel token’ları atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Türkiye Süper Lig futbol takımları hakında bilgi ver.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Türkiye Süper Lig, 1959'da kurulan Türk futbol ligi. Türkiye'de oynanan profesyonel futbol ligidir. Türkiye'de 1959-60 sezonunda başlamıştır. Süper Lig, Türkiye Futbol Federasyonu tarafından düzenlenir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "        alpaca_prompt.format(           # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"Türkiye Süper Lig futbol takımları hakında bilgi ver.\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "            \"\",                         # Giriş: Boş bırakıldı, model kendi bilgisine dayanacak\n",
    "            \"\",                         # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=1024,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Çıktıyı decode etme ve yazdırma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Üretilen token’ları insan tarafından okunabilir metne çevirir, özel token’ları atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denizhan Şahin kimdir? Denizhan Şahin kaç yaşında? Denizhan Şahin nereli?\n",
      "Denizhan Şahin kimdir? Denizhan Şahin kaç yaşında? Denizhan Şahin nereli? Denizhan Şahin biyografisi, Denizhan Şahin hayatı hakkında bilgiler...\n",
      "Denizhan Şahin kimdir?\n",
      "Denizhan Şahin (d. 1992, İstanbul), Türk oyuncu. 2015 yılında ilk oyunculuk deneyimini \"Bir Yol Bir Hikaye\" dizisindeki \"Çiğdem\" karakteri ile yaşayan Şahin, \"Gönül Yaralanışı\", \"Kardeşler\", \"Kara Korsanlar\", \"Şimşekler Ateşe Karşı\", \"Sevgili Evren\", \"Kara Ağa\", \"Kızıl Yol\", \"Çocuklar\" ve \"İstanbul'un Oğlu\" gibi dizilerde rol aldı.\n",
      "Denizhan Şahin kaç yaşında?\n",
      "Denizhan Şahin 1992 yılında doğduğu için 2022 yılında 30 yaşında.\n",
      "Denizhan Şahin nereli?\n",
      "Denizhan Şahin İstanbul'da doğdu.\n",
      "Denizhan Şahin biyografisi\n",
      "Denizhan Şahin, 1992 yılında İstanbul'da doğdu. İlk ve orta öğrenimini İstanbul'da tamamladı. Liseyi İstanbul'da tamamladıktan sonra İstanbul Üniversitesi, İletişim Fakültesi'nde eğitim gördü. 2015 yılında ilk oyunculuk deneyimini \"Bir Yol Bir Hikaye\" dizisindeki \"Çiğdem\" karakteri ile yaşayan Şahin, \"Gönül Yaralanışı\", \"Kardeşler\", \"Kara Korsanlar\", \"Şimşekler Ateşe Karşı\", \"Sevgili Evren\", \"Kara Ağa\", \"Kızıl Yol\", \"Çocuklar\" ve \"İstanbul'un Oğlu\" gibi dizilerde rol aldı.\n",
      "Denizhan Şahin hayatı hakkında bilgiler\n",
      "Denizhan Şahin, 1992 yılında İstanbul'da doğdu. İlk ve orta öğrenimini İstanbul'da tamamladı. Liseyi İstanbul'da tamamladıktan sonra İstanbul Üniversitesi, İletişim Fakültesi'nde eğitim gördü. 2015 yılında ilk oyunculuk deneyimini \"Bir Yol Bir Hikaye\" dizisindeki \"Çiğdem\" karakteri ile yaşayan Şahin, \"Gönül Yaralanışı\", \"Kardeşler\", \"Kara Korsanlar\", \"Şimşekler Ateşe Karşı\", \"Sevgili Evren\", \"Kara Ağa\", \"Kızıl Yol\", \"Çocuklar\" ve \"İstanbul'un Oğlu\" gibi dizilerde rol aldı.\n",
      "Denizhan Şahin, \"Kara Korsanlar\", \"Kara Ağa\", \"İstanbul'un Oğlu\" ve \"Çocuklar\" dizilerindeki performansı ile \"En İyi Kadın Oyuncu\" dalında Altın Kelebek Ödülleri'ne aday gösterildi.\n",
      "Denizhan Şahin, 2017 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2018 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2019 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2020 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2021 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2022 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2023 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2024 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2025 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2026 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2027 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2028 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2029 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2030 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2031 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2032 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2033 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2034 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2035 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2036 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2037 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2038 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2039 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2040 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2041 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2042 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2043 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2044 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödül\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "               # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"Denizhan Şahin kimdir?\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "                                   # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        \n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=2048,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Çıktıyı decode etme ve yazdırma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Üretilen token’ları insan tarafından okunabilir metne çevirir, özel token’ları atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sen hangi yapay zeka modelisin? Kendini tanıt.\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "               # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"Sen hangi yapay zeka modelisin? Kendini tanıt.\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "                                   # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        \n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=2048,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Çıktıyı decode etme ve yazdırma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Üretilen token’ları insan tarafından okunabilir metne çevirir, özel token’ları atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek nedir? DeepSeek, yapay zeka ve büyük veri teknolojilerini kullanarak bilgi ve bilgi kaynağı arayıcısı olarak çalışan bir teknoloji şirketidir..DeepSeek, büyük veri teknolojilerini ve yapay zeka algoritmalarını kullanarak bilgi ve bilgi kaynağı arayıcısı olarak çalışan bir teknoloji şirketidir. DeepSeek, verileri analiz ederek, bilgi araması ve analizlerde daha doğru ve verimli sonuçlar elde etmenizi sağlar. Ayrıca, şirket, kullanıcıların ihtiyaçlarına göre özelleştirilmiş veri analizi ve raporlama hizmetleri de sunar. DeepSeek, özellikle finans, sağlık, pazarlama ve güvenlik gibi alanlarda kullanılmaktadır.\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "               # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"DeepSeek nedir?\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "                                   # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        \n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=2048,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Çıktıyı decode etme ve yazdırma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Üretilen token’ları insan tarafından okunabilir metne çevirir, özel token’ları atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer  # Yeni eklenen kısım\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"model yolu\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Streamer'ı oluştur\n",
    "streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True,       # Başlangıç promptunu gösterme\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\"Denizhan Şahin kimdir?\"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate içinde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'ı parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 puanlı bir takım. Benim için 100 puanlı bir takım. Bu takımın önünde bir şey yok. Bu takımın önünde hiçbir şey yok. Benim için 100 puanlı bir takım. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oyn\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [\"Fenerbahçe nasıl bir takım? \"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate içinde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'ı parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    [\"Teknofest için bilgi verebilir misin? \"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate içinde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'ı parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Türkiye Süper Lig, 1972'de kurulan Türkiye'nin en üst düzey futbol ligi. Süper Lig, 1972-1973 sezonundan beri oynanmaktadır. 1984-85 sezonundan bu yana 18 takımın oynadığı ligde, 16 takımın oynadığı sezonlar da vardır. 1999-2000 sezonunda 20 takımla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takımın oynadığı ligde, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Süper Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir. Süper Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Millî Lig adıyla oynanmıştır.\n",
      "\n",
      "Süper Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Millî Lig adıyla oynanmıştır. Millî Lig, 1972'de kurulan Türkiye'nin en üst düzey futbol ligi. Millî Lig, 1972-1973 sezonundan beri oynanmaktadır. 1984-85 sezonundan bu yana 18 takımın oynadığı ligde, 16 takımın oynadığı sezonlar da vardır. 1999-2000 sezonunda 20 takımla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takımın oynadığı ligde, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Millî Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir. Millî Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Süper Lig adıyla oynanmıştır.\n",
      "\n",
      "Süper Lig, 1998-99 sezonu öncesinde Türkiye'de oynanan en üst seviyede profesyonel futbol ligi olan Millî Lig'de oynayan 18 takımdan oluşuyordu. 1998-99 sezonunda ise ligde 18 takım yer almıştı. 2000-01 sezonundan 2011-12 sezonuna kadar Süper Lig'de oynayan 18 takımın, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Süper Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir.\n",
      "\n",
      "Sezonlar\n",
      "1972'de kurulan Türkiye'nin en üst düzey futbol ligi. Süper Lig, 1972-1973 sezonundan beri oynanmaktadır. 1984-85 sezonundan bu yana 18 takımın oynadığı ligde, 16 takımın oynadığı sezonlar da vardır. 1999-2000 sezonunda 20 takımla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takımın oynadığı ligde, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Süper Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir. Süper Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Millî Lig adıyla oynanmıştır.\n",
      "\n",
      "1998-99 sezonu öncesinde Türkiye'de oynanan en üst seviyede profesyonel futbol ligi olan Millî Lig'de oynayan 18 takımdan oluşuyordu. 1998-99 sezonunda ise ligde 18 takım yer almıştı. 2000-01 sezonundan 2011-12 sezonuna kadar Süper Lig'de oynayan 18 takımın, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Süper Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir.\n",
      "\n",
      "Süper Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Millî Lig adıyla oynanmıştır.\n",
      "\n",
      "Süper Lig, 1972'de kurulan Türkiye'nin en üst düzey futbol ligi. Millî Lig, 1972-1973 sezonundan beri oynanmaktadır. 1984-85 sezonundan bu yana 18 takımın oynadığı ligde, 16 takımın oynadığı sezonlar da vardır. 1999-2000 sezonunda 20 takımla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takımın oynadığı ligde, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Millî Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir. Millî Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Süper Lig adıyla oynanmıştır.\n",
      "\n",
      "1998-99 sezonu öncesinde Türkiye'de oynanan en üst seviyede profesyonel futbol ligi olan Süper Lig'de oynayan 18 takımdan oluşuyordu. 1998-99 sezonunda ise ligde 18 takım yer almıştı. 2000-01 sezonundan 2011-12 sezonuna kadar Millî Lig'de oynayan 18 takımın, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Millî Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir.\n",
      "\n",
      "Millî Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Süper Lig adıyla oynanmıştır.\n",
      "\n",
      "1972'de kurulan Türkiye'nin en üst düzey futbol ligi. Millî Lig, 1972-1973 sezonundan beri oynanmaktadır. 1984-85 sezonundan bu yana 18 takımın oynadığı ligde, 16 takımın oynadığı sezonlar da vardır. 1999-2000 sezonunda 20 takımla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takımın oynadığı ligde, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Millî Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir. Millî Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Süper Lig adıyla oynanmıştır.\n",
      "\n",
      "1998-99 sezonu öncesinde Türkiye'de oynanan en üst seviyede profesyonel futbol ligi olan Millî Lig'de oynayan 18 takımdan oluşuyordu. 1998-99 sezonunda ise ligde 18 takım yer almıştı. 2000-01 sezonundan 2011-12 sezonuna kadar Millî Lig'de oynayan 18 takımın, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Millî Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir.\n",
      "\n",
      "Millî Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Süper Lig adıyla\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer  # Yeni eklenen kısım\n",
    "import torch\n",
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"model yolu\",  # Path to the base model\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Streamer'ı oluştur\n",
    "streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True,       # Başlangıç promptunu gösterme\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "        alpaca_prompt.format(           # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"Türkiye Süper Lig futbol takımları hakında bilgi ver.\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "            \"\",                         # Giriş: Boş bırakıldı, model kendi bilgisine dayanacak\n",
    "            \"\",                         # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=1024,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Generate içinde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'ı parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Model ve tokenizer'ı global olarak yükle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"model yolu\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "        self.current_text = \"\"\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.current_text += text\n",
    "        # Yeni satır veya noktaya göre böl\n",
    "        if '\\n' in text or '. ' in text:\n",
    "            self.queue.put(self.current_text)\n",
    "            self.current_text = \"\"\n",
    "        elif stream_end:\n",
    "            self.queue.put(self.current_text)\n",
    "            self.queue.put(None)  # Bitiş işareti\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "        <script>\n",
    "            function generate() {\n",
    "                const prompt = document.getElementById('prompt').value;\n",
    "                const output = document.getElementById('output');\n",
    "                output.innerHTML = '';\n",
    "                \n",
    "                const eventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "                \n",
    "                eventSource.onmessage = function(e) {\n",
    "                    if (e.data === 'DONE') {\n",
    "                        eventSource.close();\n",
    "                        return;\n",
    "                    }\n",
    "                    output.innerHTML += e.data;\n",
    "                    window.scrollTo(0, document.body.scrollHeight);\n",
    "                };\n",
    "            }\n",
    "        </script>\n",
    "        <textarea id=\"prompt\" rows=\"4\" cols=\"50\"></textarea><br>\n",
    "        <button onclick=\"generate()\">Generate</button>\n",
    "        <div id=\"output\" style=\"white-space: pre-wrap; margin-top: 20px;\"></div>\n",
    "    ''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "    \n",
    "    streamer = WebStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        queue=response_queue,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "        model.generate(\n",
    "            **inputs,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=2048,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        response_queue.put(None)  # İşlem bitti sinyali\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def event_stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None:\n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "\n",
    "    return Response(event_stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:22<00:00,  1.84s/it]\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.106.250.143:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Mar/2025 01:10:19] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:10:41] \"GET /generate?prompt=Fenerbahçe%20nasıl%20bir%20takım? HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:11:07] \"GET /generate?prompt=Süper%20lig%20nasıldır? HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:11:34] \"GET /generate?prompt=Süper%20Lig%20maçları%20skorları%20bu%20hafta HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer, StoppingCriteria\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Model ve tokenizer'ı global olarak yükle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"model yolu\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Thread yönetimi için global değişkenler\n",
    "current_stop_event = None\n",
    "stop_lock = threading.Lock()\n",
    "\n",
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_event):\n",
    "        super().__init__()\n",
    "        self.stop_event = stop_event\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_event.is_set()\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.queue.put(text)\n",
    "        if stream_end:\n",
    "            self.queue.put(None)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "        <script>\n",
    "            let currentEventSource = null;\n",
    "            \n",
    "            function generate() {\n",
    "                const prompt = document.getElementById('prompt').value;\n",
    "                const output = document.getElementById('output');\n",
    "                output.innerHTML = '';\n",
    "                \n",
    "                // Önceki bağlantıyı kapat\n",
    "                if(currentEventSource) currentEventSource.close();\n",
    "                \n",
    "                currentEventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "                \n",
    "                currentEventSource.onmessage = function(e) {\n",
    "                    if(e.data === 'DONE') {\n",
    "                        currentEventSource.close();\n",
    "                    } else {\n",
    "                        output.innerHTML += e.data;\n",
    "                        window.scrollTo(0, document.body.scrollHeight);\n",
    "                    }\n",
    "                };\n",
    "            }\n",
    "        </script>\n",
    "        <textarea id=\"prompt\" rows=\"4\" cols=\"50\"></textarea><br>\n",
    "        <button onclick=\"generate()\">Generate</button>\n",
    "        <div id=\"output\" style=\"white-space: pre-wrap; margin-top: 20px;\"></div>\n",
    "    ''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    global current_stop_event\n",
    "    \n",
    "    # Önceki işlemi durdur\n",
    "    with stop_lock:\n",
    "        if current_stop_event:\n",
    "            current_stop_event.set()\n",
    "        current_stop_event = threading.Event()\n",
    "        stop_event = current_stop_event\n",
    "    \n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "\n",
    "    streamer = WebStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        queue=response_queue,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        try:\n",
    "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                streamer=streamer,\n",
    "                max_new_tokens=1024,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                stopping_criteria=[StopGenerationCriteria(stop_event)]\n",
    "            )\n",
    "        finally:\n",
    "            response_queue.put(None)\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def event_stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None:\n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "\n",
    "    return Response(event_stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer, StoppingCriteria\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Model ve tokenizer'ı global olarak yükle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"model yolu\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Thread yönetimi için global değişkenler\n",
    "current_stop_event = None\n",
    "stop_lock = threading.Lock()\n",
    "\n",
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_event):\n",
    "        super().__init__()\n",
    "        self.stop_event = stop_event\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_event.is_set()\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.queue.put(text)\n",
    "        if stream_end:\n",
    "            self.queue.put(None)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>AI Chat</title>\n",
    "        <style>\n",
    "            :root {\n",
    "                --primary-color: #2563eb;\n",
    "                --user-bg: #3b82f6;\n",
    "                --bot-bg: #f1f5f9;\n",
    "                --text-color: #1e293b;\n",
    "            }\n",
    "\n",
    "            * {\n",
    "                box-sizing: border-box;\n",
    "                font-family: 'Segoe UI', sans-serif;\n",
    "            }\n",
    "\n",
    "            body {\n",
    "                margin: 0;\n",
    "                background: #f8fafc;\n",
    "                height: 100vh;\n",
    "                display: flex;\n",
    "                flex-direction: column;\n",
    "            }\n",
    "\n",
    "            .chat-container {\n",
    "                flex: 1;\n",
    "                max-width: 800px;\n",
    "                margin: 0 auto;\n",
    "                width: 100%;\n",
    "                padding: 20px;\n",
    "                overflow-y: auto;\n",
    "            }\n",
    "\n",
    "            .message {\n",
    "                max-width: 70%;\n",
    "                margin: 10px 0;\n",
    "                padding: 15px 20px;\n",
    "                border-radius: 20px;\n",
    "                animation: fadeIn 0.3s ease-out;\n",
    "                transform-origin: bottom;\n",
    "                position: relative;\n",
    "            }\n",
    "\n",
    "            .user-message {\n",
    "                background: var(--user-bg);\n",
    "                color: white;\n",
    "                margin-left: auto;\n",
    "                border-bottom-right-radius: 5px;\n",
    "            }\n",
    "\n",
    "            .bot-message {\n",
    "                background: var(--bot-bg);\n",
    "                color: var(--text-color);\n",
    "                margin-right: auto;\n",
    "                border-bottom-left-radius: 5px;\n",
    "            }\n",
    "\n",
    "            .typing-indicator {\n",
    "                display: inline-flex;\n",
    "                padding: 10px 15px;\n",
    "                background: var(--bot-bg);\n",
    "                border-radius: 20px;\n",
    "                margin: 5px 0;\n",
    "            }\n",
    "\n",
    "            .dot {\n",
    "                width: 8px;\n",
    "                height: 8px;\n",
    "                margin: 0 3px;\n",
    "                background: #64748b;\n",
    "                border-radius: 50%;\n",
    "                animation: bounce 1.4s infinite;\n",
    "            }\n",
    "\n",
    "            .dot:nth-child(2) {\n",
    "                animation-delay: 0.2s;\n",
    "            }\n",
    "\n",
    "            .dot:nth-child(3) {\n",
    "                animation-delay: 0.4s;\n",
    "            }\n",
    "\n",
    "            @keyframes fadeIn {\n",
    "                from {\n",
    "                    opacity: 0;\n",
    "                    transform: translateY(10px);\n",
    "                }\n",
    "                to {\n",
    "                    opacity: 1;\n",
    "                    transform: translateY(0);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            @keyframes bounce {\n",
    "                0%, 80%, 100% { \n",
    "                    transform: translateY(0);\n",
    "                }\n",
    "                40% {\n",
    "                    transform: translateY(-8px);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            .input-container {\n",
    "                background: white;\n",
    "                padding: 20px;\n",
    "                box-shadow: 0 -2px 10px rgba(0,0,0,0.05);\n",
    "            }\n",
    "\n",
    "            .input-wrapper {\n",
    "                max-width: 800px;\n",
    "                margin: 0 auto;\n",
    "                display: flex;\n",
    "                gap: 10px;\n",
    "            }\n",
    "\n",
    "            textarea {\n",
    "                flex: 1;\n",
    "                padding: 12px 16px;\n",
    "                border: 2px solid #e2e8f0;\n",
    "                border-radius: 12px;\n",
    "                resize: none;\n",
    "                font-size: 16px;\n",
    "                transition: border-color 0.2s;\n",
    "            }\n",
    "\n",
    "            textarea:focus {\n",
    "                outline: none;\n",
    "                border-color: var(--primary-color);\n",
    "            }\n",
    "\n",
    "            button {\n",
    "                background: var(--primary-color);\n",
    "                color: white;\n",
    "                border: none;\n",
    "                padding: 12px 24px;\n",
    "                border-radius: 12px;\n",
    "                cursor: pointer;\n",
    "                font-weight: 600;\n",
    "                transition: transform 0.2s, background 0.2s;\n",
    "            }\n",
    "\n",
    "            button:hover {\n",
    "                background: #1d4ed8;\n",
    "                transform: translateY(-1px);\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"chat-container\" id=\"chatContainer\">\n",
    "            <div class=\"message bot-message\">\n",
    "                <div class=\"typing-indicator\">\n",
    "                    <div class=\"dot\"></div>\n",
    "                    <div class=\"dot\"></div>\n",
    "                    <div class=\"dot\"></div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"input-container\">\n",
    "            <div class=\"input-wrapper\">\n",
    "                <textarea id=\"prompt\" placeholder=\"Mesajınızı yazın...\" rows=\"1\"></textarea>\n",
    "                <button onclick=\"generate()\">Gönder</button>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <script>\n",
    "            let currentEventSource = null;\n",
    "            const chatContainer = document.getElementById('chatContainer');\n",
    "            const textarea = document.getElementById('prompt');\n",
    "\n",
    "            // Otomatik textarea yüksekliği ayarı\n",
    "            textarea.addEventListener('input', () => {\n",
    "                textarea.style.height = 'auto';\n",
    "                textarea.style.height = textarea.scrollHeight + 'px';\n",
    "            });\n",
    "\n",
    "            function addMessage(text, isUser) {\n",
    "                const messageDiv = document.createElement('div');\n",
    "                messageDiv.className = `message ${isUser ? 'user-message' : 'bot-message'}`;\n",
    "                \n",
    "                if(!isUser) {\n",
    "                    const typingIndicator = document.querySelector('.typing-indicator');\n",
    "                    if(typingIndicator) typingIndicator.remove();\n",
    "                }\n",
    "                \n",
    "                messageDiv.innerHTML = text.replace(/\\n/g, '<br>');\n",
    "                chatContainer.appendChild(messageDiv);\n",
    "                messageDiv.scrollIntoView({ behavior: 'smooth' });\n",
    "            }\n",
    "\n",
    "            function generate() {\n",
    "                const prompt = textarea.value.trim();\n",
    "                if(!prompt) return;\n",
    "\n",
    "                // Önceki bağlantıyı kapat\n",
    "                if(currentEventSource) currentEventSource.close();\n",
    "                \n",
    "                // Kullanıcı mesajını ekle\n",
    "                addMessage(prompt, true);\n",
    "                textarea.value = '';\n",
    "                \n",
    "                // Typing indicator ekle\n",
    "                const typingDiv = document.createElement('div');\n",
    "                typingDiv.className = 'message bot-message';\n",
    "                typingDiv.innerHTML = `\n",
    "                    <div class=\"typing-indicator\">\n",
    "                        <div class=\"dot\"></div>\n",
    "                        <div class=\"dot\"></div>\n",
    "                        <div class=\"dot\"></div>\n",
    "                    </div>\n",
    "                `;\n",
    "                chatContainer.appendChild(typingDiv);\n",
    "                typingDiv.scrollIntoView({ behavior: 'smooth' });\n",
    "\n",
    "                // Yeni bağlantı oluştur\n",
    "                currentEventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "                \n",
    "                let botResponse = '';\n",
    "                currentEventSource.onmessage = (e) => {\n",
    "                    if(e.data === 'DONE') {\n",
    "                        currentEventSource.close();\n",
    "                    } else {\n",
    "                        botResponse += e.data;\n",
    "                        typingDiv.innerHTML = botResponse.replace(/\\n/g, '<br>');\n",
    "                        typingDiv.scrollIntoView({ behavior: 'smooth' });\n",
    "                    }\n",
    "                };\n",
    "            }\n",
    "\n",
    "            // Enter tuşu desteği\n",
    "            textarea.addEventListener('keydown', (e) => {\n",
    "                if(e.key === 'Enter' && !e.shiftKey) {\n",
    "                    e.preventDefault();\n",
    "                    generate();\n",
    "                }\n",
    "            });\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    ''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    global current_stop_event\n",
    "    \n",
    "    # Önceki işlemi durdur\n",
    "    with stop_lock:\n",
    "        if current_stop_event:\n",
    "            current_stop_event.set()\n",
    "        current_stop_event = threading.Event()\n",
    "        stop_event = current_stop_event\n",
    "    \n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "\n",
    "    streamer = WebStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        queue=response_queue,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        try:\n",
    "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                streamer=streamer,\n",
    "                max_new_tokens=1024,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                stopping_criteria=[StopGenerationCriteria(stop_event)]\n",
    "            )\n",
    "        finally:\n",
    "            response_queue.put(None)\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def event_stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None:\n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "\n",
    "    return Response(event_stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
