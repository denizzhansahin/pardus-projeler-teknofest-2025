{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gerçek çalışan\n",
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer, StoppingCriteria, BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Quantization ayarları\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "\n",
    "# Model ve tokenizer'ı yükle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"model yolu\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.bfloat16,\n",
    "    load_in_4bit = True,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# PEFT adaptörünü yükle\n",
    "model = PeftModel.from_pretrained(model, \"model yolu\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "\n",
    "# Thread yönetimi\n",
    "current_stop_event = None\n",
    "stop_lock = threading.Lock()\n",
    "\n",
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_event):\n",
    "        super().__init__()\n",
    "        self.stop_event = stop_event\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_event.is_set()\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.queue.put(text)\n",
    "        if stream_end:\n",
    "            self.queue.put(None)\n",
    "\n",
    "def format_alpaca_prompt(instruction):\n",
    "    return f\"\"\"Aşağıda bir görevi açıklayan bir talimat bulunmaktadır. İsteği uygun şekilde tamamlayan bir yanıt yazın.\n",
    "\n",
    "### Talimat:\n",
    "{instruction}\n",
    "\n",
    "### Yanıt:\n",
    "\"\"\"\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"tr\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "  <title>Alpaca Chat</title>\n",
    "  <style>\n",
    "    /* Stil tanımları aynı kalıyor */\n",
    "    .container { max-width: 800px; margin: 0 auto; padding: 20px; height: 100vh; display: flex; flex-direction: column; }\n",
    "    .chat-area { flex-grow: 1; overflow-y: auto; background: #fff; border-radius: 8px; padding: 20px; margin: 10px 0; }\n",
    "    .message { max-width: 75%; padding: 10px; margin: 5px 0; border-radius: 8px; }\n",
    "    .user { background: #dcf8c6; margin-left: auto; }\n",
    "    .ai { background: #f0f0f0; }\n",
    "    .input-container { display: flex; gap: 10px; }\n",
    "    textarea { flex-grow: 1; padding: 10px; border-radius: 8px; border: 1px solid #ddd; }\n",
    "    button { background: #007bff; color: white; border: none; padding: 10px 20px; border-radius: 8px; cursor: pointer; }\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"container\">\n",
    "    <h1>Alpaca Chat</h1>\n",
    "    <div id=\"chatArea\" class=\"chat-area\"></div>\n",
    "    <div class=\"input-container\">\n",
    "      <textarea id=\"prompt\" rows=\"3\" placeholder=\"Alpaca modeli için sorunuzu girin...\"></textarea>\n",
    "      <button onclick=\"sendPrompt()\">Gönder</button>\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <script>\n",
    "    let currentEventSource = null;\n",
    "\n",
    "    function sendPrompt() {\n",
    "      const prompt = document.getElementById('prompt').value.trim();\n",
    "      if (!prompt) return;\n",
    "\n",
    "      const chatArea = document.getElementById('chatArea');\n",
    "      \n",
    "      // Kullanıcı mesajını ekle\n",
    "      const userDiv = document.createElement('div');\n",
    "      userDiv.className = 'message user';\n",
    "      userDiv.textContent = prompt;\n",
    "      chatArea.appendChild(userDiv);\n",
    "\n",
    "      // AI mesaj konteyneri\n",
    "      const aiDiv = document.createElement('div');\n",
    "      aiDiv.className = 'message ai';\n",
    "      chatArea.appendChild(aiDiv);\n",
    "      \n",
    "      // Önceki bağlantıyı kapat\n",
    "      if(currentEventSource) currentEventSource.close();\n",
    "\n",
    "      // Yeni istek başlat\n",
    "      currentEventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "      \n",
    "      currentEventSource.onmessage = (e) => {\n",
    "        if(e.data === 'DONE') {\n",
    "          currentEventSource.close();\n",
    "          aiDiv.innerHTML += '<div style=\"color: #666; font-size: 0.8em\">▼ Cevap Tamamlandı</div>';\n",
    "          return;\n",
    "        }\n",
    "        aiDiv.textContent += e.data;\n",
    "        chatArea.scrollTop = chatArea.scrollHeight;\n",
    "      };\n",
    "      \n",
    "      document.getElementById('prompt').value = '';\n",
    "      chatArea.scrollTop = chatArea.scrollHeight;\n",
    "    }\n",
    "  </script>\n",
    "</body>\n",
    "</html>\n",
    "''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    global current_stop_event\n",
    "    \n",
    "    with stop_lock:\n",
    "        if current_stop_event:\n",
    "            current_stop_event.set()\n",
    "        current_stop_event = threading.Event()\n",
    "    \n",
    "    stop_event = current_stop_event\n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "\n",
    "    # Alpaca formatına dönüştür\n",
    "    formatted_prompt = format_alpaca_prompt(prompt)\n",
    "    \n",
    "    # Tokenizer ayarları\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors = \"pt\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 2048,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Streamer ayarları\n",
    "    streamer = WebStreamer(\n",
    "        tokenizer = tokenizer,\n",
    "        queue = response_queue,\n",
    "        skip_prompt = True,\n",
    "        skip_special_tokens = True,\n",
    "        clean_up_tokenization_spaces = True,\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        try:\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                streamer = streamer,\n",
    "                max_new_tokens = 2048,\n",
    "                temperature = 0.6,\n",
    "                top_p = 0.9,\n",
    "                top_k = 40,\n",
    "                do_sample = True,\n",
    "                repetition_penalty = 1.15,\n",
    "                eos_token_id = tokenizer.eos_token_id,\n",
    "                pad_token_id = tokenizer.pad_token_id,\n",
    "                stopping_criteria = [StopGenerationCriteria(stop_event)],\n",
    "            )\n",
    "        finally:\n",
    "            response_queue.put(None)\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None: \n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "    \n",
    "    return Response(stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
