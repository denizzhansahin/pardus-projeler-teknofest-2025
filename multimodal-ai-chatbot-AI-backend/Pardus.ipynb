{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9bfbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ca081",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37292287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7594c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f384593",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4448e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c46da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 2401 (8220.6 MB)\n"
     ]
    }
   ],
   "source": [
    "!pip3 cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d24064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import io\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify, send_file\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    BarkModel,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    pipeline,\n",
    "    Gemma3ForConditionalGeneration\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "# --- Flask Uygulamasını Başlatma ---\n",
    "app = Flask(__name__)\n",
    "# Geçici dosyaların yükleneceği klasörü oluştur\n",
    "if not os.path.exists('uploads'):\n",
    "    os.makedirs('uploads')\n",
    "\n",
    "# --- Genel Ayarlar ve Cihaz Belirleme ---\n",
    "# Modellerin CPU'da mı yoksa GPU'da mı çalışacağını belirle\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"Kullanılan Cihaz: {device}\")\n",
    "\n",
    "\n",
    "# --- MODELLERİ YÜKLEME ---\n",
    "# Modelleri uygulamanın başında YALNIZCA BİR KEZ yüklüyoruz.\n",
    "# Bu, her istekte modeli yeniden yüklemenin önüne geçer ve performansı ciddi şekilde artırır.\n",
    "\n",
    "print(\"Modeller yükleniyor... Bu işlem biraz zaman alabilir.\")\n",
    "\n",
    "# 1. Suno Bark Modelini Yükle (Text-to-Speech)\n",
    "print(\"Suno Bark modeli yükleniyor...\")\n",
    "bark_model_id = \"suno/bark\"\n",
    "bark_model = BarkModel.from_pretrained(bark_model_id).to(device)\n",
    "bark_processor = AutoProcessor.from_pretrained(bark_model_id)\n",
    "bark_sampling_rate = bark_model.generation_config.sample_rate\n",
    "print(\"Suno Bark modeli yüklendi.\")\n",
    "\n",
    "# 2. OpenAI Whisper Modelini Yükle (Speech-to-Text)\n",
    "print(\"OpenAI Whisper modeli yükleniyor...\")\n",
    "whisper_model_id = \"openai/whisper-large-v3\"\n",
    "whisper_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    whisper_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ").to(device)\n",
    "whisper_processor = AutoProcessor.from_pretrained(whisper_model_id)\n",
    "whisper_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=whisper_model,\n",
    "    tokenizer=whisper_processor.tokenizer,\n",
    "    feature_extractor=whisper_processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "print(\"OpenAI Whisper modeli yüklendi.\")\n",
    "\n",
    "# 3. Google Gemma Modelini Yükle (Text & Vision)\n",
    "print(\"Google Gemma modeli yükleniyor...\")\n",
    "gemma_model_id = \"model yolu\"\n",
    "gemma_model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    gemma_model_id, device_map=\"auto\"\n",
    ").eval()\n",
    "gemma_processor = AutoProcessor.from_pretrained(gemma_model_id)\n",
    "print(\"Google Gemma modeli yüklendi.\")\n",
    "\n",
    "print(\"\\nTüm modeller başarıyla yüklendi. API kullanıma hazır.\")\n",
    "\n",
    "\n",
    "# --- API ENDPOINT'LERİ ---\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Yapay Zeka Modelleri API'sine hoş geldiniz! Kullanılabilir endpoint'ler: /gemma/generate-text, /gemma/generate-from-image, /whisper/transcribe, /bark/generate-speech\"\n",
    "\n",
    "# 1. Gemma ile Metinden Metin Üretme\n",
    "@app.route('/gemma/generate-text', methods=['POST'])\n",
    "def gemma_text_generation():\n",
    "    data = request.get_json()\n",
    "    if not data or 'prompt' not in data:\n",
    "        return jsonify({\"error\": \"Lütfen 'prompt' anahtarıyla bir metin gönderin.\"}), 400\n",
    "\n",
    "    user_prompt = data['prompt']\n",
    "\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_prompt}]}\n",
    "        ]\n",
    "\n",
    "        inputs = gemma_processor.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, tokenize=True,\n",
    "            return_dict=True, return_tensors=\"pt\"\n",
    "        ).to(gemma_model.device)\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            generation = gemma_model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "            generation = generation[0][input_len:]\n",
    "\n",
    "        decoded_text = gemma_processor.decode(generation, skip_special_tokens=True)\n",
    "        return jsonify({\"response\": decoded_text})\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Model işlenirken bir hata oluştu: {str(e)}\"}), 500\n",
    "\n",
    "# 2. Gemma ile Görsel ve Metinden Metin Üretme\n",
    "@app.route('/gemma/generate-from-image', methods=['POST'])\n",
    "def gemma_vision_generation():\n",
    "    if 'image' not in request.files:\n",
    "        return jsonify({\"error\": \"Lütfen 'image' adıyla bir görsel dosyası yükleyin.\"}), 400\n",
    "\n",
    "    image_file = request.files['image']\n",
    "    prompt_text = request.form.get('prompt', 'Bu görseli detaylı bir şekilde açıkla.') # Varsayılan prompt\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_file.stream).convert(\"RGB\")\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": prompt_text}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        inputs = gemma_processor.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, tokenize=True,\n",
    "            return_dict=True, return_tensors=\"pt\"\n",
    "        ).to(gemma_model.device)\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            generation = gemma_model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "            generation = generation[0][input_len:]\n",
    "\n",
    "        decoded_text = gemma_processor.decode(generation, skip_special_tokens=True)\n",
    "        return jsonify({\"response\": decoded_text})\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Model işlenirken bir hata oluştu: {str(e)}\"}), 500\n",
    "\n",
    "# 3. Whisper ile Sesten Metin Üretme\n",
    "@app.route('/whisper/transcribe', methods=['POST'])\n",
    "def whisper_transcription():\n",
    "    if 'audio' not in request.files:\n",
    "        return jsonify({\"error\": \"Lütfen 'audio' adıyla bir ses dosyası yükleyin.\"}), 400\n",
    "\n",
    "    audio_file = request.files['audio']\n",
    "    # Dosyayı geçici olarak kaydet\n",
    "    filepath = os.path.join(\"uploads\", audio_file.filename)\n",
    "    audio_file.save(filepath)\n",
    "\n",
    "    try:\n",
    "        # Whisper pipeline'ını çalıştır\n",
    "        result = whisper_pipe(filepath, return_timestamps=True)\n",
    "        transcription = result[\"text\"]\n",
    "\n",
    "        return jsonify({\"transcription\": transcription})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Ses işlenirken bir hata oluştu: {str(e)}\"}), 500\n",
    "    finally:\n",
    "        # Geçici dosyayı sil\n",
    "        if os.path.exists(filepath):\n",
    "            os.remove(filepath)\n",
    "\n",
    "# 4. Suno Bark ile Metinden Ses Üretme\n",
    "@app.route('/bark/generate-speech', methods=['POST'])\n",
    "def bark_speech_generation():\n",
    "    data = request.get_json()\n",
    "    if not data or 'text' not in data:\n",
    "        return jsonify({\"error\": \"Lütfen 'text' anahtarıyla bir metin gönderin.\"}), 400\n",
    "\n",
    "    text_prompt = data['text']\n",
    "    # İsteğe bağlı olarak seslendirecek kişiyi de isteyebiliriz\n",
    "    voice_preset = data.get('voice_preset', 'v2/tr_speaker_2')\n",
    "\n",
    "    try:\n",
    "        # Uzun metinleri modelin işleyebileceği parçalara böl\n",
    "        max_chunk_length = 250\n",
    "        text_chunks = [text_prompt[i:i+max_chunk_length] for i in range(0, len(text_prompt), max_chunk_length)]\n",
    "\n",
    "        all_speech_outputs = []\n",
    "\n",
    "        # ...\n",
    "        for chunk in text_chunks:\n",
    "            # 1. Adım: Önce tüm girdileri CPU'da oluştur.\n",
    "            inputs = bark_processor(chunk, voice_preset=voice_preset, return_tensors=\"pt\")\n",
    "            \n",
    "            # 2. Adım: Girdilerdeki her bir tensörü TEK TEK doğru cihaza taşı.\n",
    "            # Bu, voice_preset'ten gelenlerin de taşınmasını garantiler.\n",
    "            inputs_on_device = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "            # 3. Adım: Modeli, tümü aynı cihazda olan girdilerle çalıştır.\n",
    "            speech_output = bark_model.generate(**inputs_on_device, do_sample=True)\n",
    "            \n",
    "            all_speech_outputs.append(speech_output[0].cpu().numpy())\n",
    "        # ...\n",
    "\n",
    "        # Tüm ses parçalarını birleştir\n",
    "        combined_speech_output = np.concatenate(all_speech_outputs)\n",
    "\n",
    "        # Sesi bir WAV dosyası olarak bellekte oluştur\n",
    "        buffer = io.BytesIO()\n",
    "        scipy.io.wavfile.write(buffer, rate=bark_sampling_rate, data=combined_speech_output)\n",
    "        buffer.seek(0)\n",
    "\n",
    "        # WAV dosyasını kullanıcıya gönder\n",
    "        return send_file(\n",
    "            buffer,\n",
    "            mimetype='audio/wav',\n",
    "            as_attachment=True,\n",
    "            download_name='generated_speech.wav'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Ses üretilirken bir hata oluştu: {str(e)}\"}), 500\n",
    "\n",
    "\n",
    "# --- Uygulamayı Çalıştırma ---\n",
    "if __name__ == '__main__':\n",
    "    # host='0.0.0.0' ile ağdaki diğer cihazlardan da erişilebilir hale getirilir\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
